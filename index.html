<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Apple Silicon install kuberntes" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/23/Apple%20Silicon%20install%20kuberntes/" class="article-date">
  <time class="dt-published" datetime="2024-03-23T13:02:39.630Z" itemprop="datePublished">2024-03-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/03/23/Apple%20Silicon%20install%20kuberntes/">Mac Apple Silicon M1max 安装kuberntes教程</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Mac-Apple-Silicon-M1max-安装kuberntes教程"><a href="#Mac-Apple-Silicon-M1max-安装kuberntes教程" class="headerlink" title="Mac Apple Silicon M1max 安装kuberntes教程"></a><center>Mac Apple Silicon M1max 安装kuberntes教程</center></h1><p>这里我们准备的测试环境如下：</p>
<p>Mac Apple Silicon Centos7 arm版本下载</p>
<p>链接: <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1J9a0KPac6eko0yPcaPCG6g">https://pan.baidu.com/s/1J9a0KPac6eko0yPcaPCG6g</a> 提取码: wc22 </p>
<table>
<thead>
<tr>
<th align="center">系统</th>
<th align="center">hostname</th>
<th align="center">IP</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Centos7</td>
<td align="center">el7-master-167</td>
<td align="center">192.168.26.167</td>
</tr>
<tr>
<td align="center">Centos7</td>
<td align="center">el7-node-168</td>
<td align="center">192.168.26.168</td>
</tr>
<tr>
<td align="center">Centos7</td>
<td align="center">el7-node-169</td>
<td align="center">192.168.26.169</td>
</tr>
<tr>
<td align="center">Centos7</td>
<td align="center">el7-node-184</td>
<td align="center">192.168.26.184</td>
</tr>
</tbody></table>
<p>这 4 台机器就是我们本次实验所搭建的基础环境，它们都是 Centos 7 系列的操作系统。之所以选择 Centos 7 作为操作系统，是因为大部分客户可能都还以 Centos 系列的操作系统为主在构建自己的系统。当然，这个搭建过程稍微调整一下也可以应用于其它的操作系统。</p>
<p>拿到主机后，我们先进行一次全面的更新（使用 yum update -y），因为 Centos 7 本身已经比较老旧了。所以尽可能的升级到最新版本是比较稳妥的，避免出现一些不兼容的问题，默认的源更新比较慢，可以替换使用阿里镜像源</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> /etc/yum.repos.d /etc/yum.repos.d.bak</span><br><span class="line"><span class="built_in">mkdir</span> /etc/yum.repos.d</span><br><span class="line">wget http://mirrors.aliyun.com/repo/Centos-altarch-7.repo -O /etc/yum.repos.d/CentOS-Base.repo</span><br><span class="line">yum clean all</span><br><span class="line">yum makecacke</span><br><span class="line">yum update -y</span><br></pre></td></tr></table></figure>

<p>集群的前期准备：</p>
<ol>
<li>修改四台集群设备主机名解析</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@EL7-Master-167 ~]<span class="comment"># cat /etc/hosts</span></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.26.167  el7-master-167</span><br><span class="line">192.168.26.168  el7-node-168</span><br><span class="line">192.168.26.169  el7-node-169</span><br><span class="line">192.168.26.184  el7-node-184</span><br><span class="line"></span><br><span class="line">可以用scp发送给其他台设备</span><br><span class="line">scp /etc/hosts root@192.168.26.168:/etc/hosts</span><br><span class="line">scp /etc/hosts root@192.168.26.169:/etc/hosts</span><br><span class="line">scp /etc/hosts root@192.168.26.184:/etc/hosts</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>四台设备关闭Selinux和关闭Swap</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">swapoff -a ; sed -i <span class="string">&#x27;/swap/d&#x27;</span> /etc/fstab</span><br><span class="line">setenforce 0</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>四台设备防火墙放行</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --set-default-zone=trusted</span><br></pre></td></tr></table></figure>

<p>然后，我们安装 kuberntes 集群相关组件的软件源。本次实验将使用官方 kubeadm 套件来完成搭建。因为国内的网络问题，我们选择阿里云的软件源。可以参考这个阿里云仓库的说明网页：<a target="_blank" rel="noopener" href="https://developer.aliyun.com/mirror/kubernetes%E3%80%82%E6%A0%B9%E6%8D%AE%E8%AF%B4%E6%98%8E%E7%BD%91%E9%A1%B5%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8%E5%A6%82%E4%B8%8B%E7%9A%84%E9%85%8D%E7%BD%AE%E6%9D%A5%E4%BD%9C%E4%B8%BA%E6%9C%AC%E6%AC%A1%E7%9A%84">https://developer.aliyun.com/mirror/kubernetes。根据说明网页，我们可以直接使用如下的配置来作为本次的</a> kuberntes 软件源配置：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &lt;&lt;<span class="string">EOF | tee /etc/yum.repos.d/kubernetes.repo</span></span><br><span class="line"><span class="string">[kubernetes]</span></span><br><span class="line"><span class="string">name=Kubernetes</span></span><br><span class="line"><span class="string">baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/rpm/</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=1</span></span><br><span class="line"><span class="string">gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/rpm/repodata/repomd.xml.key</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure>

<p>把上面这个配置保存到主机下的 &#x2F;etc&#x2F;yum.repos.d&#x2F;kuberntes.repo 中。然后开始下一步，安装容器引擎。我们这里选择 containerd 作为本次搭建的容器引擎，之所以选择 containerd，是因为 kuberntes 官方在1.24之后版本已经抛弃了 docker 作为容器引擎底层。所以我们的平台搭建也需要跟着官方的步伐走。当然，一些早期的 kubernernetes 集群还是使用 docker 作为底层容器引擎，但是这些平台可能会在未来被慢慢替换掉。  </p>
<p>关于 docker 在 Centos 系统的安装，同样可以参考官方的安装说明：<a target="_blank" rel="noopener" href="https://docs.docker.com/engine/install/centos/%E3%80%82%E8%BF%99%E9%87%8C%E5%B0%86%E8%BF%99%E4%B8%AA%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4%E7%AE%80%E8%A6%81%E7%9A%84%E8%AF%B4%E6%98%8E%E4%B8%80%E4%B8%8B%EF%BC%9A">https://docs.docker.com/engine/install/centos/。这里将这个安装步骤简要的说明一下：</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们先尝试卸载一下环境中已有的 docker，centos 7 自带的 docker 可能比较老旧，为了避免冲突，</span></span><br><span class="line"><span class="comment"># 我们先运行一下卸载，以防万一。</span></span><br><span class="line">yum remove docker \</span><br><span class="line">                  docker-client \</span><br><span class="line">                  docker-client-latest \</span><br><span class="line">                  docker-common \</span><br><span class="line">                  docker-latest \</span><br><span class="line">                  docker-latest-logrotate \</span><br><span class="line">                  docker-logrotate \</span><br><span class="line">                  docker-engine</span><br><span class="line">                  </span><br><span class="line"><span class="comment"># 接下来，进行安装，首先还是下载软件源的配置</span></span><br><span class="line"><span class="comment"># 安装这个软件源，我们需要使用到 yum-utils（提供了 yum-config-manager 命令），所以先要安装。</span></span><br><span class="line">yum install -y yum-utils</span><br><span class="line">yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来，我们开始安装最新的社区版本的 containerd</span></span><br><span class="line">yum install containerd.io cri-tools -y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装完成后，做一些启动配置，主要是让其可以通过开机自动启动</span></span><br><span class="line">systemctl <span class="built_in">enable</span> containerd --now</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>完成了 containerd 的安装后，接下来就开始对 containerd 进行改造，让其可以单独运行</p>
<p>一般情况下，containerd 的配置文件都在 &#x2F;etc&#x2F;containerd&#x2F;config.toml 中，我们先要将其备份，避免未来还原的时候找不到原来的配置。接下来，我们可以使用 containerd config default 就可以输出一个默认的 containerd 配置，这个配置可以保存到文件中，并复制到 &#x2F;etc&#x2F;containerd&#x2F;config.toml 中。默认配置我们还需要修改一下才能保证使用，这里大概整理几个需要修改的地方：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> /etc/containerd/config.toml /etc/containerd/config.toml.bak</span><br><span class="line">containerd config default &gt; /etc/containerd/config.toml</span><br></pre></td></tr></table></figure>

<p>pause 的仓库地址，默认的 pause 地址是 k8s.gcr.io 这个域名的，但是因为网络问题，我们无法访问到这个地址，我们需要切换为国内可用的域名。还是可以使用 aliyun 的。搜索关键字 sandbox_image 就可以找到这个 pause 的修改点。我们使用 registry.aliyuncs.com&#x2F;google_containers&#x2F;pause:3.9 作为 containerd 的 pause 配置。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">old:sandbox_image = <span class="string">&quot;registry.k8s.io/pause:3.9&quot;</span></span><br><span class="line">new:sandbox_image = <span class="string">&quot;registry.aliyuncs.com/google_containers/pause:3.9&quot;</span></span><br></pre></td></tr></table></figure>

<p>Systemd Cgroup 相关的配置，containerd 默认不使用 sytemd cgroup 作为自身容器的 cgroup 管理依赖。这样会导致系统运行可能存在不稳定的情况，所以我们需要将这个配置打开，让 containerd 一开始就运行在 systemd 的 cgroup 下，这样保证系统的整体稳定。但是，这里修改还是需要注意一个细节，一些比较老的版本（比如 containerd 1.4.3 版本），这个修改点在 systemd_cgroup 这个关键字上，将其改为 true 就可以了。但是在比较新的版本（比如 containerd 1.5.11 版本或者更新的 1.6.x 版本）。这个配置就移动到了另外的位置（通过搜索 runc.options 或者 SystemdCgroup 这些关键字），特别是对于 1.5.11 版本来说，如果配置了 SystemdCgroup 这个为 true，就不要配置 systemd_cgroup 为 true 了。可能会导致 containerd 无法正常启动。而对于 1.6.x 版本来说，SystemdCgroup 和 systemd_cgroup 可以同时为 true，并且 containerd 可以正常启动。但是这个会导致 crictl 工作不正常。所以，在较新的 containerd 版本上，只配置 SystemdCgroup 就可以了。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">old:SystemdCgroup = <span class="literal">false</span></span><br><span class="line">new:SystemdCgroup = <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>接下来，我们可以为一些主流的公共仓库配置一些镜像加速器，通过搜索 registry.mirrors 关键字，找到修改点，并加入相关的镜像加速器地址。修改一定要注意对齐</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">old:</span><br><span class="line">[plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.mirrors]</span><br><span class="line"></span><br><span class="line">new:</span><br><span class="line">[plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.mirrors]</span><br><span class="line">  [plugins.<span class="string">&quot;io.containerd.grpc.v1.cri&quot;</span>.registry.mirrors.<span class="string">&quot;docker.io&quot;</span>]</span><br><span class="line">    endpoint = [<span class="string">&quot;https://registry-1.docker.io&quot;</span>, <span class="string">&quot;https://registry.docker-cn.com&quot;</span>, <span class="string">&quot;http://hub-mirror.c.163.com&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>最后，我们看看 containerd 应用了新配置后，重新启动containerd。正常启动后，还需要配置 crictl 这个工具。具体是通过配置 &#x2F;etc&#x2F;crictl.yaml 来完成。上述流程的相关配置，可以参考这些配置文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">可以将以上修改的配置文件用scp发送给其他设备</span><br><span class="line">scp /etc/containerd/config.toml root@192.168.26.168:/etc/containerd/config.toml</span><br><span class="line">scp /etc/containerd/config.toml root@192.168.26.169:/etc/containerd/config.toml</span><br><span class="line">scp /etc/containerd/config.toml root@192.168.26.184:/etc/containerd/config.toml</span><br><span class="line">重启服务</span><br><span class="line">systemctl restart containerd ; systemctl <span class="built_in">enable</span> containerd</span><br></pre></td></tr></table></figure>

<p>配置crictl</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &lt;&lt; <span class="string">EOF | sudo tee /etc/crictl.yaml</span></span><br><span class="line"><span class="string">runtime-endpoint: unix:///run/containerd/containerd.sock</span></span><br><span class="line"><span class="string">image-endpoint: unix:///run/containerd/containerd.sock</span></span><br><span class="line"><span class="string">timeout: 10</span></span><br><span class="line"><span class="string">debug: false</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure>

<p>完成了上述配置后，现在我们使用 crictl version 应该就可以打印出相关信息了。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@EL7-Master-167 ~]<span class="comment"># crictl version</span></span><br><span class="line">Version:  0.1.0</span><br><span class="line">RuntimeName:  containerd</span><br><span class="line">RuntimeVersion:  1.6.28</span><br><span class="line">RuntimeApiVersion:  v1</span><br></pre></td></tr></table></figure>

<p>但是使用crictl命令十分麻烦，可以使用nerdctl代替，命令类似docker</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">下载链接</span><br><span class="line">https://github.com/containerd/nerdctl/releases/tag/v1.7.5</span><br><span class="line">https://github.com/containernetworking/plugins/releases</span><br><span class="line"></span><br><span class="line">安装nerdctl插件</span><br><span class="line">tar zxf nerdctl-1.7.5-linux-arm64.tar.gz -C /usr/bin/</span><br><span class="line"><span class="built_in">mkdir</span> -p /opt/cni/bin/</span><br><span class="line">tar zxf cni-plugins-linux-arm64-v1.4.1.tgz -C /opt/cni/bin/</span><br><span class="line"><span class="built_in">mkdir</span> /etc/nerdctl/</span><br><span class="line"></span><br><span class="line"><span class="built_in">cat</span> &gt; /etc/nerdctl/nerdctl.toml &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">debug          = false</span></span><br><span class="line"><span class="string">debug_full     = false</span></span><br><span class="line"><span class="string">address        = &quot;unix:///run/containerd/containerd.sock&quot;</span></span><br><span class="line"><span class="string">namespace      = &quot;k8s.io&quot;</span></span><br><span class="line"><span class="string">#snapshotter    = &quot;stargz&quot;</span></span><br><span class="line"><span class="string">cgroup_manager = &quot;systemd&quot;</span></span><br><span class="line"><span class="string">#hosts_dir      = [&quot;/etc/containerd/certs.d&quot;, &quot;/etc/nerdctl/certs.d&quot;]</span></span><br><span class="line"><span class="string">insecure_registry = true</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line">loginctl show-user root | grep Linger</span><br><span class="line">loginctl enable-linger root</span><br><span class="line"></span><br><span class="line"><span class="built_in">cat</span> &lt;&lt;<span class="string">EOF &gt; /etc/sysctl.d/k8s.conf </span></span><br><span class="line"><span class="string">net.bridge.bridge-nf-call-ip6tables = 1 </span></span><br><span class="line"><span class="string">net.bridge.bridge-nf-call-iptables = 1 </span></span><br><span class="line"><span class="string">net.ipv4.ip_forward = 1 </span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line">sysctl -p /etc/sysctl.d/k8s.conf</span><br></pre></td></tr></table></figure>

<p>接下来，我们来安装 kubernetes 的部署组件。这里我们指定使用 1.28.8 这个版本。我们可以先使用 yum –showduplicates list kubeadm 来看看哪些版本我们可以安装，确认我们指定安装的版本是否在列表中。当我们发现了指定的版本存在后，就可以直接进行安装了。安装的时候指定相应的版本：yum install -y kubeadm-1.28.8 kubelet-1.28.8 kubectl-1.28.8。安装完成后，我们为了防止系统未来自动升级更新到最新的 kubeadm 版本，需要进行一下版本锁定。锁定的方法是先安装 yum-versionlock，然后使用命令 yum versionlock kubeadm kubectl kubelet 来进行锁定。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install -y kubeadm-1.28.8 kubelet-1.28.8 kubectl-1.28.8</span><br><span class="line">systemctl restart kubelet ; systemctl <span class="built_in">enable</span> kubelet</span><br></pre></td></tr></table></figure>

<p>接下来，我们可以先使用 kubeadm config images list 看看这个版本所依赖的组件有哪些。主要还是对比一下 pause 版本和之前在 containerd 中配置的 pause 版本是否一致。到这里，用于安装 kubernetes 集群的软件基础就准备好了。现在还需要调整一下主机系统的配置，使其适合用于 kubernetes 运行。 </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@EL7-Master-167 ~]<span class="comment"># kubeadm config images list</span></span><br><span class="line">registry.k8s.io/kube-apiserver:v1.28.0</span><br><span class="line">registry.k8s.io/kube-controller-manager:v1.28.0</span><br><span class="line">registry.k8s.io/kube-scheduler:v1.28.0</span><br><span class="line">registry.k8s.io/kube-proxy:v1.28.0</span><br><span class="line">registry.k8s.io/pause:3.9</span><br><span class="line">registry.k8s.io/etcd:3.5.9-0</span><br><span class="line">registry.k8s.io/coredns/coredns:v1.10.1</span><br></pre></td></tr></table></figure>

<p>相对 ubuntu 操作系统来说，centos 系列要安装 kubernetes 之前所需要做的调整会多一些。具体来说有如下几条：</p>
<ol>
<li>关闭 selinux，这个 selinux 是 centos 系统默认自带的，但是官方推荐关闭 selinux 后安装 kubernetes 集群，因为 selinux 配置太过于复杂，可能会导致 kubernetes 出现运行冲突。具体关闭的方法是先 setenforce 0 临时关闭 selinux，这个时候 selinux 仅仅只是运行在 Permissive 模式，下次重启还会生效。如果要完全关闭 selinux，那么就需要修改 &#x2F;etc&#x2F;selinux&#x2F;config 下的配置，将 SELINUX&#x3D;enforcing 改为 SELINUX&#x3D;disabled，然后重启系统。重启完成后，再使用 getenforce 来查看，就是 Disabled 模式了。  </li>
<li>关闭防火墙，防火墙会阻止集群节点间的通信，如果对相关端口非常熟悉的情况下，可以通过 firewall 进行端口打开。但是只是一个测试集群的话，可以先关闭防火墙搭建。具体方法是 systemctl stop firewalld &amp;&amp; systemctl disable firewalld。</li>
<li>对于 Swap 的配置，一般情况下，官方要求部署 kubernetes 的时候关闭节点的 Swap 功能，这样做的目的是避免性能问题。但是随着技术的发展，现在 Swap 也慢慢不再是降低系统运行性能的主要问题。并且，再大多数实际运行中，打开 Swap 功能可以有效的缓冲集群对内存的大量使用，节约宝贵的内存资源。这里搭建测试环境由于本身内存也比较小，就不再关闭 Swap 分区。</li>
<li>修改节点 hostname，一般来说，kubernetes 的集群中节点的名称就是主机的 hostname，但是这个名称也可以不一样。这里建议还是保持一致，这样无论是观察集群情况，还是用于主机和集群之间的通信建立都更加的方便和统一。具体的方法就是将主机节点的 hostname 进行修改（直接修改 &#x2F;etc&#x2F;hostname 可以使其持久化），和 kuberntes 的 node name 保持一致。</li>
<li>设定开机自动加载 ipvs 驱动，本次测试环境的搭建，我们使用 kube-proxy 的 ipvs 模式。早期 kube-proxy 使用的是 iptables 来做 Service 的负载均衡，由于 Pod 的频繁变动，导致 kube-proxy 在删除旧规则和添加新规则上存在巨大的性能开销。而后面 ipvs 模式的出现就很好的缓解了这个性能问题。ipvs 模式下，kube-proxy 将负载均衡的配置移动到 ipvs 中去完成，这样大大减轻了设置 iptables 带来的系统压力。具体如何在节点上配置 ipvs 的能力，首先需要安装管理工具：yum install ipvsadm -y，然后是加载相关驱动，为了实现开机自动加载，可以创建一个 &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules 脚本（不要忘记加上执行权限），在这个脚本中写入如下代码执行加载动作。</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/modules/ipvs.modules </span><br><span class="line">ipvs_mods_dir=<span class="string">&quot;/usr/lib/modules/<span class="subst">$(uname -r)</span>/kernel/net/netfilter/ipvs&quot;</span></span><br><span class="line"><span class="keyword">for</span> mod <span class="keyword">in</span> $(<span class="built_in">ls</span> <span class="variable">$ipvs_mods_dir</span> | grep -o <span class="string">&quot;^[^.]*&quot;</span>)</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">        /usr/sbin/modinfo -F filename <span class="variable">$mod</span> &amp;&gt; /dev/null</span><br><span class="line">        <span class="keyword">if</span> [ $? -eq 0 ];</span><br><span class="line">        <span class="keyword">then</span></span><br><span class="line">                /usr/sbin/modprobe <span class="variable">$mod</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">chmod</span> u+x /etc/sysconfig/modules/ipvs.modules</span><br></pre></td></tr></table></figure>

<p>到这里，关于 kuberntes 集群系统的一些准备工作基本上就完成了。我们可以使用 kubeadm 开始进行集群部署的工作。因为部署的过程中，我们涉及到了一些自定义的配置，而命令行参数无法完全满足配置的要求。所以我们需要通过配置文件的方式来部署。首先，我们使用 kubeadm config print init-defaults &gt; kubeadm_init.yaml 创建一个初始化的 init 配置文件。我们要在这个配置文件中，添加一些 kubelet 的配置，然后也会对 kube-apiserver 以及 kube-proxy 的组件进行一些定制化的设置。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeadm.k8s.io/v1beta3</span></span><br><span class="line"><span class="attr">bootstrapTokens:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">groups:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">system:bootstrappers:kubeadm:default-node-token</span></span><br><span class="line">  <span class="attr">token:</span> <span class="string">abcdef.0123456789abcdef</span></span><br><span class="line">  <span class="attr">ttl:</span> <span class="string">24h0m0s</span></span><br><span class="line">  <span class="attr">usages:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">signing</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">authentication</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">InitConfiguration</span></span><br><span class="line"><span class="attr">localAPIEndpoint:</span></span><br><span class="line">  <span class="attr">advertiseAddress:</span> <span class="number">192.168</span><span class="number">.26</span><span class="number">.167</span></span><br><span class="line">  <span class="attr">bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">nodeRegistration:</span></span><br><span class="line">  <span class="attr">criSocket:</span> <span class="string">/run/containerd/containerd.sock</span></span><br><span class="line">  <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">EL7-Master-167</span></span><br><span class="line">  <span class="attr">taints:</span> </span><br><span class="line">  <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;node-role.kubernetes.io/master&quot;</span></span><br><span class="line">    <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiServer:</span></span><br><span class="line">  <span class="attr">timeoutForControlPlane:</span> <span class="string">4m0s</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeadm.k8s.io/v1beta3</span></span><br><span class="line"><span class="attr">certificatesDir:</span> <span class="string">/etc/kubernetes/pki</span></span><br><span class="line"><span class="attr">clusterName:</span> <span class="string">wenjunk8s</span></span><br><span class="line"><span class="attr">controllerManager:</span> &#123;&#125;</span><br><span class="line"><span class="attr">dns:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">CoreDNS</span></span><br><span class="line"><span class="attr">etcd:</span></span><br><span class="line">  <span class="attr">local:</span></span><br><span class="line">    <span class="attr">dataDir:</span> <span class="string">/var/lib/etcd</span></span><br><span class="line"><span class="attr">imageRepository:</span> <span class="string">registry.aliyuncs.com/google_containers</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterConfiguration</span></span><br><span class="line"><span class="attr">kubernetesVersion:</span> <span class="number">1.28</span><span class="number">.8</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line">  <span class="attr">dnsDomain:</span> <span class="string">cluster.local</span></span><br><span class="line">  <span class="attr">serviceSubnet:</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.0</span><span class="string">/12</span></span><br><span class="line"><span class="attr">scheduler:</span> &#123;&#125;</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeproxy.config.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeProxyConfiguration</span></span><br><span class="line"><span class="attr">mode:</span> <span class="string">ipvs</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubelet.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeletConfiguration</span></span><br><span class="line"><span class="attr">address:</span> <span class="string">&quot;0.0.0.0&quot;</span></span><br><span class="line"><span class="attr">cgroupDriver:</span> <span class="string">systemd</span></span><br><span class="line"><span class="attr">cgroupsPerQOS:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">serializeImagePulls:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">evictionHard:</span></span><br><span class="line">  <span class="attr">memory.available:</span> <span class="string">&quot;100Mi&quot;</span></span><br><span class="line">  <span class="attr">nodefs.available:</span> <span class="string">&quot;10%&quot;</span></span><br><span class="line">  <span class="attr">nodefs.inodesFree:</span> <span class="string">&quot;5%&quot;</span></span><br><span class="line">  <span class="attr">imagefs.available:</span> <span class="string">&quot;10%&quot;</span></span><br><span class="line"><span class="attr">failSwapOn:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>在上面这个配置中，我们可以看到在 InitConfiguration 中，我们指定了 criSocket 为 &#x2F;run&#x2F;containerd&#x2F;containerd.sock，并且把当前应用这个初始化配置的节点取名为 EL7-Master-167，然后还配置了一个污点（不过 kubeadm 默认也会打上一个污点，这个污点配置主要是为了兼容老平台的配置）。</p>
<p>在 ClusterConfiguration 配置中，然后，我们还指定了组件拉取的仓库 imageRepository 为 registry.aliyuncs.com&#x2F;google_containers，这样可以保证我们能顺利拉取相应的镜像部署集群。当然，我们也不能忘记在配置中通过 kubernetesVersion 指定好需要部署的 kuberntes 版本。</p>
<p>在 KubeProxyConfiguration 配置中，我们需要将 kube-proxy 的模式设定为 ipvs，这样部署的时候 kubeadm 才能写入指定的配置参数。  </p>
<p>最后一个 KubeletConfiguration 配置，这个配置是比较重要的，我们通过添加 failSwapOn: false 的参数使得 kubelet 在启动的时候忽略对主机是否使用 Swap 分区的检查，这样 kubelet 才能正常启动。其它的一些配置例如和 cgroup 相关的配置，都是根据自己的需求写入。上面的示例只是一个推荐配置，就是部署的时候最好关注一下相关的配置是否满足要求。</p>
<p>在完成了配置文件的编写后，我们就可以直接使用 kubeadm init –config .&#x2F;kubeadm_init.yaml 进行初始化。如果一切顺利，master 就会被顺利启动起来，并在终端中输出如下的说明：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  <span class="built_in">mkdir</span> -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  sudo <span class="built_in">cp</span> -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  sudo <span class="built_in">chown</span> $(<span class="built_in">id</span> -u):$(<span class="built_in">id</span> -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">Alternatively, <span class="keyword">if</span> you are the root user, you can run:</span><br><span class="line"></span><br><span class="line">  <span class="built_in">export</span> KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run <span class="string">&quot;kubectl apply -f [podnetwork].yaml&quot;</span> with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">Then you can <span class="built_in">join</span> any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm <span class="built_in">join</span> 192.168.26.167:6443 --token abcdef.0123456789abcdef \</span><br><span class="line">        --discovery-token-ca-cert-hash sha256:1ef67f719f2ce0c5cdc2375d72bbba22d773f6fb0fdca73d1cdc90282a874bff </span><br></pre></td></tr></table></figure>

<p>根据这个输出，我们除了知道 master 已经部署成功了之外。还需要知道接下来我们要做的事情。首先，我们需要配置 kubectl 所使用的证书，具体方法就是在当前用户的 $HOME 目录下建立 .kube 这个文件夹，然后将 &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf 这个文件复制到 $HOME&#x2F;.kube 这个文件夹中，并以 config 进行重新命名。当然，你也可以不去选择这个配置方式，通过环境变量是一样的。只是个人认为这个比较麻烦。下一步，就是应用 join 的命令到其它的节点。一般情况下，运行命令就好了。只不过，我们本次搭建对 node 节点也有自定义的配置要求，所以我们还是采取通过配置文件的方式来完成。</p>
<p>我们先还是使用 kubeadm config print join-defaults &gt; kubeadm_join.yaml 来生成一个默认的 join 配置，然后在这个基础上进一步的修改添加自己所需要的定制化配置：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeadm.k8s.io/v1beta3</span></span><br><span class="line"><span class="attr">caCertPath:</span> <span class="string">/etc/kubernetes/pki/ca.crt</span></span><br><span class="line"><span class="attr">discovery:</span></span><br><span class="line">  <span class="attr">bootstrapToken:</span></span><br><span class="line">    <span class="attr">apiServerEndpoint:</span> <span class="number">192.168</span><span class="number">.26</span><span class="number">.167</span><span class="string">:6443</span>     <span class="string">//kube-apiserver实际地址</span></span><br><span class="line">    <span class="attr">token:</span> <span class="string">abcdef.0123456789abcdef</span></span><br><span class="line">    <span class="attr">caCertHashes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">sha256:1ef67f719f2ce0c5cdc2375d72bbba22d773f6fb0fdca73d1cdc90282a874bff</span> </span><br><span class="line">  <span class="attr">timeout:</span> <span class="string">5m0s</span></span><br><span class="line">  <span class="attr">tlsBootstrapToken:</span> <span class="string">abcdef.0123456789abcdef</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">JoinConfiguration</span></span><br><span class="line"><span class="attr">nodeRegistration:</span></span><br><span class="line">  <span class="attr">criSocket:</span> <span class="string">&quot;var/run/containerd/containerd.sock&quot;</span></span><br><span class="line">  <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">el7-node-168</span>  <span class="string">//修改加入节点hostname</span></span><br><span class="line">  <span class="attr">taints:</span> <span class="literal">null</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubelet.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeletConfiguration</span></span><br><span class="line"><span class="attr">address:</span> <span class="string">&quot;0.0.0.0&quot;</span></span><br><span class="line"><span class="attr">cgroupDriver:</span> <span class="string">systemd</span></span><br><span class="line"><span class="attr">cgroupsPerQOS:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">serializeImagePulls:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">evictionHard:</span></span><br><span class="line">  <span class="attr">memory.available:</span> <span class="string">&quot;100Mi&quot;</span></span><br><span class="line">  <span class="attr">nodefs.available:</span> <span class="string">&quot;10%&quot;</span></span><br><span class="line">  <span class="attr">nodefs.inodesFree:</span> <span class="string">&quot;5%&quot;</span></span><br><span class="line">  <span class="attr">imagefs.available:</span> <span class="string">&quot;10%&quot;</span></span><br><span class="line"><span class="attr">failSwapOn:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>我们在 join 的配置中，主要还是新增了关于 kubelet 的定制配置，这个配置和 master 上的 init 配置保持一致。其它的则是 JoinConfiguration 的配置，在这个配置中，我们主要就是将 init 结束的时候输出的 join 命令参数填写到了这个配置中的不同条目里。这里需要注意一下 node 节点通过 master 加入到集群的时候。一开始是通过 bootstrap token 建立双向信任关系。然后进行了初始化配置后，再切换为证书加密通信的方式。</p>
<p>检查完配置确认没有问题后，我们就使用 kubeadm join –config .&#x2F;kubeadm_join.yaml 来执行 join 的流程。这样，我们基本上就已经完成集群建设的大半工作了。我们可以通过 kubectl get node 看看是不是所有的节点都已经加入到集群中，然后开始下一步的配置。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">节点依次执行</span><br><span class="line">kubeadm <span class="built_in">join</span> --config ./kubeadm_join.yaml</span><br></pre></td></tr></table></figure>

<p>接下来，我们需要部署 CNI 插件，完成了 CNI 插件后，整个集群才会具备一个基本的容器间网络。这里选择 calico，因为部署过程相对比较简单，对系统也没有太多的特殊要求。我们可以参考官方文档 <a target="_blank" rel="noopener" href="https://docs.tigera.io/calico/latest/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico-with-kubernetes-api-datastore-50-nodes-or-less">https://docs.tigera.io/calico/latest/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico-with-kubernetes-api-datastore-50-nodes-or-less</a> 中的说明，不过在部署前，我们还要对这个部署文件进行检查。主要是看看 POD 网络的网段是否可能和主机的网段冲突。默认情况下，calico 会使用一个 192.168.0.0&#x2F;16 的网段作为自己分配 POD 地址的网段，但是如果自己的主机恰好也是使用这个网段的话，那么就需要调整以避开这些网段。具体方法是通过修改 CALICO_IPV4POOL_CIDR 这个参数来完成，这个参数以环境变量的形式挂载到 calico 容器中。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CALICO_IPV4POOL_CIDR</span></span><br><span class="line">  <span class="attr">value:</span> <span class="string">&quot;10.244.0.0/16&quot;</span></span><br></pre></td></tr></table></figure>

<p>在部署的过程中，还有一个小插曲，就是测试环境对于 dockerhub 的访问也有问题。为了避免拉取问题，这里将 calico 相关镜像提前拉取下来，然后修改名称后上传到公司的阿里云公共仓库上。接下来，还要进行 imagePullSecrets 的创建，因为公司的阿里云公共仓库是需要认证才能下载的。这里我们拉取镜像使用只读账户即可，如果后续有上传需求，可以请 @邱明杰 提供可读写的认证账户。</p>
<p>接下来，实际部署过程就很简单了，这里将具体步骤整理如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们首先通过远程仓库下载 yaml 文件。</span></span><br><span class="line">curl https://raw.githubusercontent.com/projectcalico/calico/v3.26.4/manifests/calico.yaml -O</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改配置文件，添加 CALICO_IPV4POOL_CIDR 相关的配置变更，导入本地镜像</span></span><br><span class="line">- name: CALICO_IPV4POOL_CIDR</span><br><span class="line">  value: <span class="string">&quot;10.244.0.0/16&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 防止镜像拉取失败可以本地导入四个节点</span></span><br><span class="line">nerdctl load -i calico_images\:v3.26.4.tar </span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后，我们直接 kubectl apply -f 这个文件</span></span><br><span class="line">kubectl apply -f calico.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来就查看 kube-system 就可以看到这个 pod 资源被创建了</span></span><br><span class="line">kubectl -n kube-system get pod</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>完成了 CNI 插件的部署后，还需要一个管理工具，这里推荐安装一个官方的管理工具 calicoctl 。可以参考这个官方文档 <a target="_blank" rel="noopener" href="https://docs.tigera.io/calico/latest/operations/calicoctl/install#install-calicoctl-as-a-kubectl-plugin-on-a-single-host">https://docs.tigera.io/calico/latest/operations/calicoctl/install#install-calicoctl-as-a-kubectl-plugin-on-a-single-host</a> 完成安装。这里选择是让 calicoctl 直接成为 kubectl 的子命令。实现方法也很简单，直接将 calicoctl 下载下来，然后放到 master 主机的 $PATH 目录下，以特定的 kubectl- 这个前缀命令。接下来就可以直接使用 kubectl calicol -h 看看是否安装成功了。下面是部署所使用的相关资源：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">部署使用Linux arm64</span><br><span class="line">curl -L https://github.com/projectcalico/calico/releases/download/v3.27.2/calicoctl-linux-arm64 -o kubectl-calico</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$PATH</span></span><br><span class="line">/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin</span><br><span class="line"></span><br><span class="line"><span class="built_in">mv</span> kubectl-calico /usr/bin/kubectl-calico</span><br><span class="line"></span><br><span class="line"><span class="built_in">chmod</span> +x /usr/bin/kubectl-calico</span><br><span class="line"></span><br><span class="line">kubectl calico -h</span><br></pre></td></tr></table></figure>

<p>完成了 CNI 插件的部署后，接下来我们需要部署一个 metrics server，这个 metrics server 部署完成后，我们不仅可以使用 kubectl top 命令查看集群内部节点或 pod 的资源消耗，还可以用于支持配置 HPA 等自动化伸缩的功能。我们可以通过参考官方项目 <a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/metrics-server">https://github.com/kubernetes-sigs/metrics-server</a> 上的说明来完成部署。首先，我们还是获取到最新的部署脚本：curl -L -k -v -O <a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml%E3%80%82%E7%84%B6%E5%90%8E%E4%B8%BA%E4%BA%86%E6%88%90%E5%8A%9F%E7%9A%84%E9%83%A8%E7%BD%B2%EF%BC%8C%E6%88%91%E4%BB%AC%E9%9C%80%E8%A6%81%E5%AF%B9%E5%85%B6%E5%81%9A%E4%B8%80%E4%BA%9B%E6%94%B9%E9%80%A0%E3%80%82%E5%85%B6%E6%AC%A1%E5%B0%B1%E6%98%AF%E5%9C%A8">https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml。然后为了成功的部署，我们需要对其做一些改造。其次就是在</a> metrics server 的启动参数中添加 –kubelet-insecure-tls 这个参数，添加这个参数的原因在于 metrics server 会从 kubelet 中获取相应的性能信息，但是这个获取的过程默认情况下需要使用证书进行认证。但是 metrics server 默认的部署脚本中，对这个证书挂载处理的不是很好，所以为了避免问题，我们直接添加 –kubelet-insecure-tls 参数来取消 metrics server 和 kubelet 之间严格的证书校验。还有一个配置细节是需要在 kube-apiserver 的启动参数中添加 –enable-aggregator-routing&#x3D;true 的配置（这个在前面部署 kuberntes master 时已经有过提及），这个配置可以防止 metrics server 运行时不断的输出 loading OpenAPI spec for “v1beta1.metrics.k8s.io“ failed with: failed to retrieve 的错误日志（–enable-aggregator-routing&#x3D;true 参数用于启用聚合层API服务器的路由功能。当设置为 true 时，它允许聚合层中的API服务器将请求路由到自定义 API 服务，从而允许这些自定义 API 服务为 Kubernetes 集群添加自定义功能。这个参数通常用于扩展 Kubernetes API 并实现定制化的功能，而 metrics server 恰好就是使用 aggregator 模型来聚合 API 请求的实现，在较新版本的 Kubernetes 中，默认情况下可能已经启用了聚合路由，不过我们依然可以显式指定来确保万无一失）。下面是部署所使用的相关资源：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提前导入本地镜像</span></span><br><span class="line">nerdctl load -i metrics-server-v0.7.0.tar</span><br></pre></td></tr></table></figure>

<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">metrics-server</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">metrics-server</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">metrics-server</span></span><br><span class="line">    <span class="attr">rbac.authorization.k8s.io/aggregate-to-admin:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">rbac.authorization.k8s.io/aggregate-to-edit:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">rbac.authorization.k8s.io/aggregate-to-view:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:aggregated-metrics-reader</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">metrics.k8s.io</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">pods</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">nodes</span></span><br><span class="line">  <span class="attr">verbs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">metrics-server</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:metrics-server</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">nodes/metrics</span></span><br><span class="line">  <span class="attr">verbs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">pods</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">nodes</span></span><br><span class="line">  <span class="attr">verbs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">metrics-server</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">metrics-server-auth-reader</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">extension-apiserver-authentication-reader</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">metrics-server</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">metrics-server</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">metrics-server:system:auth-delegator</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:auth-delegator</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">metrics-server</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">metrics-server</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:metrics-server</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:metrics-server</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">metrics-server</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">metrics-server</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">metrics-server</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">https</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">443</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="string">https</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">metrics-server</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">metrics-server</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">metrics-server</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">metrics-server</span></span><br><span class="line">  <span class="attr">strategy:</span></span><br><span class="line">    <span class="attr">rollingUpdate:</span></span><br><span class="line">      <span class="attr">maxUnavailable:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">metrics-server</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">imagePullSecrets:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">sdaliyunregistry</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">args:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--cert-dir=/tmp</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--secure-port=4443</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--kubelet-insecure-tls</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--kubelet-use-node-status-port</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--metric-resolution=15s</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">registry.k8s.io/metrics-server/metrics-server:v0.7.0</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/livez</span></span><br><span class="line">            <span class="attr">port:</span> <span class="string">https</span></span><br><span class="line">            <span class="attr">scheme:</span> <span class="string">HTTPS</span></span><br><span class="line">          <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">metrics-server</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">4443</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">https</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="attr">readinessProbe:</span></span><br><span class="line">          <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/readyz</span></span><br><span class="line">            <span class="attr">port:</span> <span class="string">https</span></span><br><span class="line">            <span class="attr">scheme:</span> <span class="string">HTTPS</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">20</span></span><br><span class="line">          <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">200Mi</span></span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">false</span></span><br><span class="line">          <span class="attr">readOnlyRootFilesystem:</span> <span class="literal">true</span></span><br><span class="line">          <span class="attr">runAsNonRoot:</span> <span class="literal">true</span></span><br><span class="line">          <span class="attr">runAsUser:</span> <span class="number">1000</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/tmp</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">tmp-dir</span></span><br><span class="line">      <span class="attr">nodeSelector:</span></span><br><span class="line">        <span class="attr">kubernetes.io/os:</span> <span class="string">linux</span></span><br><span class="line">      <span class="attr">priorityClassName:</span> <span class="string">system-cluster-critical</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">metrics-server</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">emptyDir:</span> &#123;&#125;</span><br><span class="line">        <span class="attr">name:</span> <span class="string">tmp-dir</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apiregistration.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">APIService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">metrics-server</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">v1beta1.metrics.k8s.io</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">group:</span> <span class="string">metrics.k8s.io</span></span><br><span class="line">  <span class="attr">groupPriorityMinimum:</span> <span class="number">100</span></span><br><span class="line">  <span class="attr">insecureSkipTLSVerify:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">service:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">metrics-server</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">v1beta1</span></span><br><span class="line">  <span class="attr">versionPriority:</span> <span class="number">100</span></span><br></pre></td></tr></table></figure>

<p>完成了 metrics server 的部署后，我们这个集群基本上就已经是一个可用的集群了。不过，为了后续的云原生平台搭建，我们还需要做一些准备工作。下面来介绍一下目前 kuberntes 下常用的安装包管理工具 helm 的安装和使用。helm 对于 kuberntes 来说就如同 apt 或 yum 对于 ubuntu 或者 centos 来说的角色。在 kuberntes 中部署一个可以使用的应用，需要涉及到很多 kuberntes 资源的共同协作。而这些 kuberntes 资源过于分散，不方便进行管理，所以我们需要一个更加方便的管理工具，helm 就是这样一个工具。目前，helm 已经被大多数云原生化组件所采用。下面，我们来安装 helm 工具，helm 工具访问集群也需要使用证书，所以一般可以将 helm 和 kubectl 放在一个节点上。我们通过官方提供的脚本来获取 helm：curl -o get_helm.sh <a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3%E3%80%82%E4%B8%8B%E8%BD%BD%E5%AE%8C%E6%88%90%E5%90%8E%EF%BC%8C%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B8%AA%E8%84%9A%E6%9C%AC%EF%BC%8C%E5%B0%B1%E5%8F%AF%E4%BB%A5%E5%AE%8C%E6%88%90">https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3。下载完成后，执行这个脚本，就可以完成</a> helm 的安装工作。接下来可以在终端中运行 helm –help 看看这个工具是否已经可以正常工作。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3</span><br><span class="line">bash get_helm.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果虚拟机无法下载，本地上传复制到$PATH赋予执行权限即可</span></span><br><span class="line"><span class="built_in">cp</span> helm /usr/bin/helm</span><br><span class="line"><span class="built_in">chmod</span> +x /usr/bin/helm</span><br></pre></td></tr></table></figure>

<p>部署完成 helm 后，我们将使用 helm 再部署几个中间件，用于支撑后面云原生应用的部署。首先，我们来部署一个 ingress 组件，这个 ingress 组件的部署主要是为 kuberntes 集群提供 ingress 的使用能力。这里我们选择 ingress-nginx 进行部署，具体可以参考官方的文档：<a target="_blank" rel="noopener" href="https://kubernetes.github.io/ingress-nginx/%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BD%BF%E7%94%A8%E5%85%B6%E6%8F%90%E4%BE%9B%E7%9A%84">https://kubernetes.github.io/ingress-nginx/，我们使用其提供的</a> helm 仓库来进行部署。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加一个 ingress nginx 的仓库</span></span><br><span class="line">helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新仓库信息</span></span><br><span class="line">helm repo update</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载最新的 ingress nginx 部署脚本</span></span><br><span class="line">helm pull ingress-nginx/ingress-nginx</span><br><span class="line">tar -xzvf ingress-nginx-4.10.0.tgz </span><br><span class="line"></span><br><span class="line"><span class="comment"># 在四台节点都导入镜像</span></span><br><span class="line">nerdctl load -i ingress-nginx-controller-v1.10.0.tar </span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来，则是打开 default-backend 的配置（这个 default backend 的作用主要就是用于返回 404 的响应）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 namespace，一些特定的组件最好使用不同的 namespace 进行部署，这样方便管理和做租户访问权限控制。</span></span><br><span class="line">kubectl create namespace ingress-nginx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行部署</span></span><br><span class="line"> helm --debug --namespace ingress-nginx upgrade --install --force --cleanup-on-fail wenjunk8s-proxy ./ingress-nginx</span><br></pre></td></tr></table></figure>

<p>部署命令执行完成后，我们可以通过 kubectl -n ingress-nginx get pod 看看 POD 的运行状态。等待全部启动完成后，我们就可以在集群中使用 ingress 了。还需要注意的是，比较新的 kuberntes 版本中，ingress 组件都会绑定到指定的 ingress class 中，这样做的好处就是可以在这个集群中部署多套 ingress 组件。下面是部署所使用的相关资源：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<p>部署完 ingress 组件后，我们还要为云原生应用考虑一个持久化存储中间件。在 kubernetes 集群中，由于 POD 会出现漂移，这里我们需要部署一个存储服务器来提供集群内网络存储的挂载。可以将这个网络存储理解为 kubernetes 集群的云盘。我们不仅要部署一个网络存储，还要使得这个存储服务可以对接到 kuberntes 中的 StorageClass 编排资源。这样我们才能更加方便的在云原生应用部署时更方便的引用。</p>
<p>这里我们选择 NFS 服务器作为这个测试集群的网络存储。由于 NFS 服务器本身是一个运行在单节点上的存储服务器，所以我们部署的时候需要指定一个节点作为 NFS 服务器的运行节点。这里，我们选择 el7-node-168（也就是 192.168.26.168）作为 NFS 服务器的运行节点。接下来，我们还需要对主机节点做一定的改造，加载 NFS 服务器相关的驱动，在各个节点上安装 NFS 的客户端（kuberntes 集群中的 POD 挂载网络存储的原理就是通过 kubelet 操作主机上的 NFS 客户端，将网络存储挂载到指定的 POD 中）。</p>
<p>配置主机开机自动加载 NFS 服务器驱动加载，我们可以选择在 &#x2F;etc&#x2F;modules-load.d 这个文件夹中创建相关驱动加载的配置，比如我们创建 nfs.conf、nfsd.conf 以及 nfsv4.conf 这几个配置文件，并在其中分别写入 nfs、nfsd 以及 nfsv4 这几个驱动。这样开机的时候就能够完成驱动的自动加载。可以通过 lsmod | grep nfs 来检查一下结果，如果加载成功就可以过滤出相关的驱动输出。当然，考虑到后面的扩展性，也可以其它的节点上也这样配置，这样尽可能的将整个集群的节点存储都利用起来。还有一个配置细节是，这个 NFS 服务器的编排资源部署，我们需要配置 kube-apiserver 打开 –feature-gates&#x3D;”RemoveSelfLink&#x3D;false” 的参数，这样来避免控制器本身报错 pvc error getting claim reference: selfLink was empty, can‘t make refere。这是因为在一些早期实现的控制器中，可能还会用编排资源中 selflink 相关的特性。但是在较新的 kubernetes 集群中，已经移除了 selflink 相关的特性支持，所以为了兼容老的控制器，我们需要将这个特性打开。在上述准备工作完成后，接下来我们开始具体的部署：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<p>部署完成后，我们需要配置整个集群的主机都可以访问到这个服务。由于我们只需要在集群内使用，而集群内的节点本身是可以直接通过 Service 的 ClusterIP 访问到 Pod 的。那么，我们可以获取到这个 NFS 创建的 Service 的 ClusterIP，然后配置主机的 &#x2F;etc&#x2F;hosts 添加域名和和地址的映射：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<p>现在，集群本身就拥有了为 POD 提供持久化存储的能力了。下面是 nfs 的相关部署资源：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line"></span><br><span class="line">最后，我们将要部署 cert-manager，这个组件是帮助我们自动化签发 Ingress HTTPS 证书的组件。在云原生应用的部署中，我们有时候需要对外提供 web 服务，而这些服务一般都通过 HTTPS 的方式来提供以提高通信的安全性。如果每一个应用我们都手动签发证书，那么工作量将会非常的大，并且十分容易出错，这里我们借助 cert-manager 自动化的签发证书，降低云原生应用部署的难度。</span><br><span class="line"></span><br><span class="line">部署的时候，需要关注一个细节，不同版本的 cert-manager 存在对 kuberntes 的版本支持差异。也就是说，不同版本的 kubernetes 只能使用对应版本的 cert-manager。我们本次部署的是 1.28.8 的 kuberntes 集群，那么可以使用 1.12.0 版本的 cert-manager 进行部署。下面一个部署的步骤：</span><br><span class="line"></span><br><span class="line">```Bash</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>下面是部署 cert-manager 所使用的编排资源：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">部署成功后，就可以创建 issuer 了，关于 issuer 有两种类型可以选择。一种是只在 namespace 下生效的 Issuer，另外一种则是在全局生效的 ClusterIssuer。我们可以选择两个都部署，并且将这个 issuer 部署为自签名的模式。</span><br><span class="line"></span><br><span class="line">```bash</span><br></pre></td></tr></table></figure>



<p>创建这个自签名证书的时候，我们可以使用通配符的方式来匹配带有某个后缀的所有域名，这样进行云原生应用部署的时候，我们只需要保持域名开头的不同就可以使用这个证书了。同时，为了避免证书过早过期的问题，我们可以使用长一点的时间。</p>
<p>目前来说，我们的 kuberntes 集群已经初步具备了一个云原生常见的能力了。我们可以在上面批量发布标准化的云原生应用，为用户提供对外的访问接口。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/03/23/Apple%20Silicon%20install%20kuberntes/" data-id="clu43xcl800001ou72cp7dnkx" data-title="Mac Apple Silicon M1max 安装kuberntes教程" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/23/hello-world/" class="article-date">
  <time class="dt-published" datetime="2024-03-23T12:39:58.011Z" itemprop="datePublished">2024-03-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/03/23/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/03/23/hello-world/" data-id="clu42wlye000069u7d3jw0rwk" data-title="Hello World" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/03/23/Apple%20Silicon%20install%20kuberntes/">Mac Apple Silicon M1max 安装kuberntes教程</a>
          </li>
        
          <li>
            <a href="/2024/03/23/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>